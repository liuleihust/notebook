Redis 可以用来做什么？

> 存储中间件，它是 Remote Dictionary Service 的缩写，就是 远程字典服务。

Redis 的用途：

1. 缓存
2. 分布式锁

##### Redis 用在什么地方

1. 记录帖子的点赞数、评论数和点击数
2. 记录用户的帖子列表，便于快速显示
3. 缓存近期热帖内容，减少数据库压力
4. ..

## Redis  基础数据结构

Redis 有 5 种基础数据结构，分别为 string, list (列表),set(集合),hash,zset(有序集合)。

#### String (字符串)

Redis 中所有的数据结构都以唯一的 **key 字符串**作为名称，不同类型的数据结构的差异在于*value*的结构不一样。

字符串结构使用非常广泛，一个常见的用途就是缓存用户信息，将用户信息结构体使用JSON 序列化成字符串。将序列化的字符串塞进 Redis 来缓存。

Redis 的字符串是动态字符串，内部结构实现类似于 Java 的 Arraylist,采用 预分配冗余空间的方式来减少内存的频繁分配。当字符串长度小于1M时，扩容是加倍现有的空间，如果超过1M，扩容一次只会扩1M的空间。字符串最大长度 512M.



```shell
// 设置键值对
set name liulei
get name 

// 批量设置，节省网络耗时开销
mset name1 a name2 b name3 c
mget name1 name2 name3


//过期 set 命令扩展
// 可以对 key 设置过期时间，到点自动删除
set name liulie
expire name 5 //5s 后过期

setex name 5 liulei // 等价于 set+expire

setnx name liulei // 如果name 不存在就执行 set 创建

// 计数，如果value 值是一个整数，可以对它进行自增操作

set age 30
incr age; // 自增1 ，不会清除过期时间
incrby age 5 // 增加5
incrby age -5 // 减少5


```

字符串由 多个字节组成，每个字节又是由 8 个 bit 组成，如此可以看成很多 bit 组合。

#### list(列表)

Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)，当列表弹出最后一个元素后，该数据结构自动被删除，内存被回收。

Redis 的列表结构可以用来做 异步队列使用，将需要延后处理的任务结构体序列化字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。

```java
// 右进
rpush books 1 2 3

// list len
 llen books 
 
 // 左 出 
 lpop books
 // 右出
 rpop books
 
 // 慢操作，在 list 查找某个 元素,需要对 链表进行遍历
 lindx books 1 // index 从 0 开始。-1 表示 倒数第一个元素，
 
  // list range [0,1]
   lrange books 0 1
    
 // ltrim : index in [start end ] retain
    ltrim books 0 0 
    lrange books 0 -1 // 只有第一个元素
    
    
 
```

Redis 底层存储的不是一个简单的 linkenlist , 而是快速链表 quicklist 的一个结构

首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 `ziplist` ，也就是压缩列表。当数据量比较多的时候才会改 成*quicklist* 。因为普通的链表**需要的附加指针空间太大**，会比较浪费空间，而且会加重内存的碎片化。比如这个列表里存的只是 `int` 类型的数据，结构上还需要两个额外的指针 `prev` 和 `next` 。所以 Redis 将链表和 `ziplist` 结合起来组成了 `quicklist`。也就是将多个 `ziplist` 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。

#### hash 字典

Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。



不同的是Redis 的字典的值只能是字符串，另外它们 rehash 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略。

渐进式 rehash 会在 rehash 的同时，保留新旧两个 hash 结构，查询时会同时查询两个 hash 结构，然后在**后续的定时任务中以及 hash 操作指令中**，循序渐进地将旧 hash 的内容一点点迁移到新的 hash 结构中。当搬迁完成了，就会使用新的hash结构取而代之。

![img](https://user-gold-cdn.xitu.io/2018/7/28/164dc873b2a899a8?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

> **rehash 过程** 
>
> 一个 hash value 有两个 ht[2] 指向 hashtable(hash 表)，当需要扩容的时候，创建一个大小为扩容后的 hashtable (里面没有键值对),并让ht[1]指向这个新的hashtable。接下来在每次 查询中，查询两个 hashtable,如果查找的键值对在 ht[0]中，则将 这个键值对移到 ht[1]中，当ht[0]中没有键值对的时候，让 ht[0] 指向 这个新的 hashtable,h[1]=null。

**当 hash 移除了最后一个元素之后，该数据结构自动被删除，内存被回收。**

hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对用户结构中的每个字段单独存储。**这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取，这样就会比较浪费网络流量。**

hash 也有缺点，hash 结构的**存储消耗要高于单个字符串**，到底该使用 hash 还是字符串，需要根据实际情况再三权衡。

```shell
> hset books java "think in java"  # 命令行的字符串如果包含空格，要用引号括起来
(integer) 1
> hset books golang "concurrency in go"
(integer) 1
> hset books python "python cookbook"
(integer) 1
> hgetall books  # entries()，key 和 value 间隔出现
1) "java"
2) "think in java"
3) "golang"
4) "concurrency in go"
5) "python"
6) "python cookbook"
> hlen books
(integer) 3
> hget books java
"think in java"
> hset books golang "learning go programming"  # 因为是更新操作，所以返回 0
(integer) 0
> hget books golang
"learning go programming"
> hmset books java "effective java" python "learning python" golang "modern golang programming"  # 批量 set
OK

// hash 也可以 对 value中的单个子key 进行计数

hincrby student age 1
```

#### set(集合)

Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，**字典中所有的 value 都是一个值**`NULL`。

set 结构可以用来存储活动中奖的用户 ID，**因为有去重功能**，可以保证同一个用户不会中奖两次。

```shell
> sadd books python
(integer) 1
> sadd books python  #  重复
(integer) 0
> sadd books java golang
(integer) 2
> smembers books  # 注意顺序，和插入的并不一致，因为 set 是无序的
1) "java"
2) "python"
3) "golang"
> sismember books java  # 查询某个 value 是否存在，相当于 contains(o)
(integer) 1
> sismember books rust
(integer) 0
> scard books  # 获取长度相当于 count()
(integer) 3
> spop books  # 弹出一个
"java"
```

#### zset(有序集合)

zset 可能是 Redis 提供的最为特色的数据结构，它也是在面试中面试官最爱问的数据结构。它类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 `set`，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。它的内部实现用的是一种叫做「跳跃列表」的数据结构。

zset 可以用来存粉丝列表，value 值是粉丝的用户 ID，score 是关注时间。我们可以对粉丝列表按关注时间进行排序。

zset 还可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。

```shell
> zadd books 9.0 "think in java"
(integer) 1
> zadd books 8.9 "java concurrency"
(integer) 1
> zadd books 8.6 "java cookbook"
(integer) 1
> zrange books 0 -1  # 按 score 排序列出，参数区间为排名范围 默认是升序
1) "java cookbook"
2) "java concurrency"
3) "think in java"
> zrevrange books 0 -1  # 按 score 逆序列出，参数区间为排名范围
1) "think in java"
2) "java concurrency"
3) "java cookbook"
> zcard books  # 相当于 count()
(integer) 3
> zscore books "java concurrency"  # 获取指定 value 的 score
"8.9000000000000004"  # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题
> zrank books "java concurrency"  # 排名
(integer) 1
> zrangebyscore books 0 8.91  # 根据分值区间遍历 zset
1) "java cookbook"
2) "java concurrency"
> zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。
1) "java cookbook"
2) "8.5999999999999996"
3) "java concurrency"
4) "8.9000000000000004"
> zrem books "java concurrency"  # 删除 value
(integer) 1
> zrange books 0 -1
1) "java cookbook"
2) "think in java"
```

##### 跳跃列表

zset 内部的排序功能是通过 跳跃列表数据结构来实现的。因为 zset 支持随机的插入和删除，所以它不好使用数组来表示。

我们需要一个链表按照 score 值进行排序，意味着当有新元素需要插入的时候，需定位到特定位置的插入点。通常我们通过二分查找来找到插入点，但是普通的链表不支持二分查找，所以对链表进行改造。

跳跃列表就是类似层级制，最下面一层所有的元素都会串起来。然后每隔几个元素挑选出一个代表来，再将这几个代表使用另外一级指针串起来，然后在这些代表里再挑出二级代表，再串起来。最终形成了金字塔结构。

![img](https://user-gold-cdn.xitu.io/2018/7/23/164c5bb13c6da230?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

跳跃列表之所以 跳跃，是因为内部的元素可能身兼数职，一个元素同时处于 L0 L1和L2层，可以快速在不同层次之间进行跳跃。

在插入新元素时，先在定层进行定位，然后下潜到下一级定位，一直下潜到最底层找到合适的位置，将新元素插进去。

插入的新元素 采取随机策略来决定新元素可以兼职到第几层。

跳跃列表类似 平衡二叉树的方式寻找插入点，它比平衡二叉树的缺点是：当插入一个新元素时，可能修改指针过多，优点是 获取某个区间的所有元素更快。

#### 容器型数据结构的通用规则

1. **create if not exists**
2. **drop if no elements**

#### 过期时间

当一个key 设置了过期时间，然后调用了set 方法修改它， 它的过期时间会消失。

## 分布式锁

比如一个操作要修改 用户的状态，修改状态需要先读出用户的状态，在内存里进行修改，然后再存回去。这样的操作 不是原子的。（所谓的原子操作就是指不会被线程调度机制打断的操作，操作一旦开始，就一直运行到结束，中间不会有任何 context switch 线程切换）

分布式锁本质是要实现的目标先在 Redis 里面占一个茅坑，当别的进程也要来占时，发现已经有人蹲在那里了，就只好放弃或者稍后再试。

占坑一般先使用 setnx 指令，如果不存在，则设置 并返回1 ，否则返回0。

释放锁就是 调用 del 指令。

```shell
// 这里的冒号:就是一个普通的字符，没特别含义，它可以是任意其它字符，不要误解
> setnx lock:codehole true
OK
... do something critical ...
> del lock:codehole
(integer) 1
```

存在一个问题：

如果逻辑执行到中间出现异常了，会导致 del 指令没有被调用，这样就会陷入死锁，锁永远得不到释放。

于是我们在拿到锁之后，再给锁加上一个过期时间，比如 5s，这样即使中间出现异常也可以保证 5 秒之后锁会自动释放。

但是以上逻辑还有问题。如果在 setnx 和 expire 之间服务器进程突然挂掉了，可能是因为机器掉电或者是被人为杀掉的，就会导致 expire 得不到执行，也会造成死锁。

这种问题的根源就在于 setnx 和 expire 是两条指令而不是原子指令。如果这两条指令可以一起执行就不会出现问题

Redis 加入了 set 指令的扩展参数，使得 setnx和 expire 指令可以一起执行。

```shell
> set lock:codehole true ex 5 nx
OK
... do something critical ...
> del lock:codehole
```

#### 超时问题

Redis 的分布式锁 不能解决超时问题，如果在加锁和释放锁之间逻辑执行的太长，以至于超出了锁的超时限制，就会出现问题。因为这时候第一个线程持有的锁过期了，临界区的逻辑还没有执行完，这个时候第二个线程就提前重新持有了这把锁，导致临界区代码不能得到严格的串行执行。而且有可能第一个线程把第二个线程的锁释放掉。

为了避免这个问题，**Redis 分布式锁**不要用于较长时间的任务。如果真的偶尔出现了，数据出现的小波错乱可能需要人工介入解决。

有一个 **稍微安全一点**  的方案是 为了 set 指令的 value 参数设置为一个随机数，释放锁时先匹配随机数是否一致，然后再删除 key，这是为了确保当前线程占用的锁不会被其他线程释放。

但是它仍然会出现超时问题。

#### 可重入性

可重入性是指线程在持有锁的情况下再次请求加锁，如果一个锁支持同一个线程的多次加锁，那么这个锁就是可重入的。比如 Java 语言里有个 ReentrantLock 就是可重入锁。Redis 分布式锁如果要支持可重入，**需要对客户端的 set 方法进行包装，使用线程的 Threadlocal 变量存储当前持有锁的计数。**

但是这样会加重客户端的复杂性，不推荐使用。

#### Redis 的分布式锁真的安全吗？

> 背景：
>
> Redis开发者 提出一个 基于Redis 的分布式锁的实现，Redlock ，分布式系统专家 MK 认为存在问题。

##### 基于单Redis 节点的分布式锁

首先，Redis 客户端为了获取锁，向Redis 节点发送命令：

```shell
SET resource_name my_random_value NX PX 30000
```

如果上述命令执行成功，则客户端成功获取锁。否则，执行失败。

当客户端完成了对共享资源的操作后，执行Redis Lua 脚本来释放锁：因为 Lua 脚本可以实现一系列操作的原子性。

```java
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

这段Lua脚本在执行的时候要把前面的`my_random_value`作为`ARGV[1]`的值传进去，把`resource_name`作为`KEYS[1]`的值传进去。

单节点的Redis 分布式锁 无法解决的问题是 ：

如果 redis 节点 宕机了，那么所有客户端就无法获得锁了，服务变的不可用。为了提高可用性，我们可以给这个Redis 节点挂一个 Slave ，当Master 节点不可用的时候，系统自动切到 Slave 上，但是Redis 的主从复制是异步的，这就可能导致 在 failover(失效备援)过程中丧失锁的安全性。

即 两个用户获得同一个锁。

##### Redlock 算法

由于前面介绍的基于单Redis节点的分布式锁在failover的时候会产生解决不了的安全性问题，因此antirez提出了新的分布式锁的算法Redlock，它基于N个完全独立的Redis节点（通常情况下N可以设置成5）。

运行Redlock算法的客户端依次执行下面各个步骤，来完成**获取锁**的操作：

1. 获取当前时间（毫秒数）。
2. 按顺序依次向N个Redis节点执行**获取锁**的操作。这个获取操作跟前面基于单Redis节点的**获取锁**的过程相同，包含随机字符串`my_random_value`，也包含过期时间(比如`PX 30000`，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个**获取锁**的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。**这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有**（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。
3. 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（>= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)（如果超过了，有可能第一个获取的锁已经失效了），那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
4. 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。
5. 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起**释放锁**的操作（即前面介绍的Redis Lua脚本）。

释放锁的过程比较简单：客户端向所有Redis 节点发起释放锁的操作，不管这些节点在当时在获取锁的时候成功与否。

如果有的节点发生崩溃重启，还是会对锁的安全性有影响。具体的影响程度跟Redis 对数据的持久化程度有关。

假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：

1. 客户端1成功锁住了A, B, C，**获取锁**成功（但D和E没有锁住）。
2. 节点C崩溃重启了，但客户端1在C上加的锁**没有持久化下来**，丢失了。
3. 节点C重启后，客户端2锁住了C, D, E，**获取锁**成功。

默认情况下，Redis 的持久化方式是每秒写一次磁盘（即执行fsync），因此最坏的情况下可能丢失1秒的数据。可以修改为每次修改数据都进行 fsync，但这会降低性能。

为了应对这一问题，redis 作者提出了 **延迟重启** 概念，也就是说，一个节点崩溃后，先不立即重启它，等待一段时间（这段时间应该大于锁的有效时间）后重启。

**为什么释放锁的时候向所有节点发生请求呢？**

这是考虑有可能redis 获取锁请求成功 的响应包 丢失了。

客户端长期阻塞导致锁过期，使得共享资源不安全在 Redlock 依然存在。

另外，在算法第4步成功获取了锁之后，如果由于获取锁的过程消耗了较长时间，重新计算出来的剩余的锁有效时间很短了，那么我们还来得及去完成共享资源访问吗？如果我们认为太短，是不是应该立即进行锁的释放操作？那到底多短才算呢？又是一个选择难题。

##### MK 的分析

MK 主要提出了两个问题

-  客户端超时引起锁失效，但是客户端并没有发现锁失效了(可能进程pause或网络延迟)，引起访问共享资源 的不安全性。
- 认为Redlock 本质上是建立在一个同步模型之上，对系统的记时假设（timing assumiption）有很强的要求，因此本身的安全性不够。

针对第一个问题，MK提出了 fencing token ，即获取锁的同时加一个 token(token 依次递增)。

客户端访问共享资源的时候带着 token,共享资源的服务可以对其进行检查，拒绝掉延迟到来的访问请求。

第二个问题：

Martin在文中构造了一些事件序列，能够让Redlock失效（两个客户端同时持有锁）。为了说明Redlock对系统记时(timing)的过分依赖，他首先给出了下面的一个例子（还是假设有5个Redis节点A, B, C, D, E）：

1. 客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。
2. 节点C上的**时钟发生了向前跳跃**，导致它上面维护的锁快速过期。
3. 客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。
4. 客户端1和客户端2现在都认为自己持有了锁。

本质上是因为Redlock的安全性(safety property)对系统的时钟有比较强的依赖，**一旦系统的时钟变得不准确**，算法的安全性也就保证不了了。

在Martin的这篇文章中，还有一个很有见地的观点，就是对锁的用途的区分。他把锁的用途分为两种：

- 为了效率(efficiency)，协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的email。
- 为了正确性(correctness)。在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致(inconsistency)，数据丢失，文件损坏，或者其它严重的问题。

最后，Martin得出了如下的结论：

- 如果是为了效率(efficiency)而使用分布式锁，允许锁的偶尔失效，那么使用单Redis节点的锁方案就足够了，简单而且效率高。Redlock则是个过重的实现(heavyweight)。
- 如果是为了正确性(correctness)在很严肃的场合使用分布式锁，那么不要使用Redlock。它不是建立在异步模型上的一个足够强的算法，它对于系统模型的假设中包含很多危险的成分(对于timing)。而且，它没有一个机制能够提供fencing token。那应该使用什么技术呢？Martin认为，应该考虑类似Zookeeper的方案，或者支持事务的数据库。



##### Redis 作者的反驳

他 对 MK 提出的两方面进行反驳

-  既然在锁失效的情况下已经存在一种 fencing 机制能继续保证资源的互斥访问，为什么还要使用分布式锁呢？（分布式锁比单点Redis 消耗大，并且fencing 机制也不安全）
- 对于时钟跳跃的情况（比如：系统管理员手动修改了时钟；从NTP服务收到一个大的时钟更新事件），可以避免。而且Redlock 对时钟的要求，并不需要完全精确，只需要时钟差不多精确就可以了。

回想一下Redlock算法的具体过程，它使用起来的过程大体可以分成5步：

1. 获取当前时间。
2. 完成**获取锁**的整个过程（与N个Redis节点交互）。
3. 再次获取当前时间。
4. 把两个时间相减，计算**获取锁**的过程是否消耗了太长时间，导致锁已经过期了。如果没过期，
5. 客户端持有锁去访问共享资源。

如果在第三步 之后 发生延迟，即客户端拿到了一个 它认为有效而实际却已经过期了的锁。

他认为这种情况在所有分布式锁的实现是普遍存在的。

> Redis 的分布式锁 存在安全问题的因素主要是 **过期时间**。

#### 基于ZooKeeper 的分布式锁更安全吗？

很多人（也包括Martin在内）都认为，如果你想构建一个更安全的分布式锁，那么应该使用ZooKeeper，而不是Redis。那么，为了对比的目的，让我们先暂时脱离开本文的题目，讨论一下基于ZooKeeper的分布式锁能提供绝对的安全吗？它需要fencing token机制的保护吗？

一个基于ZooKeeper构建分布式锁的描述（当然这不是唯一的方式）：

- 客户端尝试创建一个znode节点，比如`/lock`。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode已存在），获取锁失败。
- 持有锁的客户端访问共享资源完成后，将znode删掉，这样其它客户端接下来就能来获取锁了。
- znode应该被创建成ephemeral的。这是znode的一个特性，它保证如果创建znode的那个客户端崩溃了，那么相应的znode会被自动删除。这保证了锁一定会被释放。

看起来这个锁相当完美，没有Redlock过期时间的问题，而且能在需要的时候让锁自动释放。但仔细考察的话，并不尽然。

*ZooKeeper*是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与ZooKeeper的某台服务器维护着一个Session，这个*Session*依赖定期的心跳(heartbeat)来维持。如果ZooKeeper长时间收不到客户端的心跳（这个时间称为Sesion的过期时间），那么它就认为Session过期了，通过这个Session所创建的所有的ephemeral类型的znode节点都会被自动删除。

设想如下的执行序列：

1. 客户端1创建了znode节点`/lock`，获得了锁。
2. 客户端1进入了长时间的GC pause。
3. 客户端1连接到ZooKeeper的Session过期了。znode节点`/lock`被自动删除。
4. 客户端2创建了znode节点`/lock`，从而获得了锁。
5. 客户端1从GC pause中恢复过来，**它仍然认为自己持有锁**。

最后，客户端1和客户端2都认为自己持有了锁，冲突了。这与之前Martin在文章中描述的由于GC pause导致的分布式锁失效的情况类似。即只有当客户端发送崩溃时锁才有效。

总结一下，基于ZooKeeper的锁和基于Redis的锁相比在实现特性上有两个不同：

- 在正常情况下，客户端可以持有锁任意长的时间，这可以确保它做完所有需要的资源访问操作之后再释放锁。这避免了基于Redis的锁对于有效时间(lock validity time)到底设置多长的两难问题。实际上，基于ZooKeeper的锁是依靠Session（心跳）来维持锁的持有状态的，而Redis不支持Sesion。
- 基于ZooKeeper的锁支持在获取锁失败之后等待锁重新释放的事件（watch 通知）。这让客户端对锁的使用更加灵活。

#### Redis  分布式锁遇到锁冲突如何处理

1. 直接抛出异常，通知用户稍后重试
2. sleep 一会再重试
3. 将请求转移至延时队列，过一会再试。

#### 延时队列的实现

延时队列可以通过 Redis 的 zset(有序列表) 来实现。我们将消息序列化成一个字符串作为 zset 的`value`，这个消息的到期处理时间作为`score`，然后用多个线程轮询 zset **获取到期的任务**进行处理，多个线程是为了保障可用性，万一挂了一个线程还有其它线程可以继续处理。因为有多个线程，所以需要考虑并发争抢任务，确保任务不能被多次执行。

## 位图

一个 bit 能存储 bool 型数据，所有用 位图来存储用户一年的签到记录，或者统计月活的时候，因为需要去重，所以使用 set 来记录所有活跃用户的 id,非常浪费内存。这时就可以使用位图来标记用户的活跃状态。不过这个方法是由条件的，那就是 userid 是整数连续的，并且活跃占比较高。

## HyperLogLog

如果网站要统计每天的 UV （独立访客）数据，怎么做？

同一个用户一天之内的多次访问请求只能计数一次。这就要求每一个网页请求都需要带上用户的 ID，无论是登陆用户还是未登陆用户都需要一个唯一 ID 来标识。

一个简单的方案是为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。当一个请求过来时，我们使用 sadd 将用户ID 塞进来就可以 。通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV数据。

但是，如果你的页面访问量非常大，比如一个爆款页面几千万的 UV，你需要一个很大的 set 集合来统计，这就非常浪费空间。如果这样的页面很多，那所需要的存储空间是惊人的。为这样一个去重功能就耗费这样多的存储空间，值得么？其实老板需要的数据又不需要太精确，105w 和 106w 这两个数字对于老板们来说并没有多大区别，So，有没有更好的解决方案呢？

HyperLogLog 数据结构是 Redis 的高级数据结构，它非常有用，但是令人感到意外的是，使用过它的人非常少。

### 使用方法

HyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加计数，一个是获取计数。pfadd 用法和 set 集合的 sadd 是一样的，来一个用户 ID，就将用户 ID 塞进去就是。pfcount 和 scard 用法是一样的，直接获取计数值。

```shell
127.0.0.1:6379> pfadd codehole user1
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 1
127.0.0.1:6379> pfadd codehole user2
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 2
127.0.0.1:6379> pfadd codehole user3
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 3
127.0.0.1:6379> pfadd codehole user4
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 4
127.0.0.1:6379> pfadd codehole user5
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 5
127.0.0.1:6379> pfadd codehole user6
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 6
127.0.0.1:6379> pfadd codehole user7 user8 user9 user10
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 10
```

### 注意事项

HyperLogLog 这个数据结构不是免费的，不是说使用这个数据结构要花钱，它需要占据一定 12k 的存储空间，所以它不适合统计单个用户相关的数据。如果你的用户上亿，可以算算，这个空间成本是非常惊人的。但是相比 set 存储方案，HyperLogLog 所使用的空间那真是可以使用千斤对比四两来形容了。

不过你也不必过于担心，因为 Redis 对 HyperLogLog 的存储进行了优化，在计数比较小时，它的存储空间采用稀疏矩阵存储，空间占用很小，仅仅在计数慢慢变大，稀疏矩阵占用空间渐渐超过了阈值时才会一次性转变成稠密矩阵，才会占用 12k 的空间。

### HyperLogLog 实现原理

依据统计和概率，给定一系列的随机整数，我们记录下低位连续零位的最大长度 k,通过这个k 值可以估算出随机数的数量。

通过实验可以发现 K和N 的对数存在线性相关性；

```shell
N=2^K  # 约等于
```

如果 N 介于 2^K 和 2^(K+1) 之间，用这种方式估计的值都等于 2^K，这明显是不合理的。这里可以采用多个 BitKeeper，然后进行加权估计，就可以得到一个比较准确的值。

使用 1024 个桶，计算平均数使用了调和平均 (倒数的平均)。普通的平均法可能因为个别离群值对平均结果产生较大的影响，调和平均可以有效平滑离群值的影响。然后将平均数*1024

### pf 为什么占用 12k

我们在上面的算法中使用了 1024 个桶进行独立计数，不过在 Redis 的 HyperLogLog 实现中用到的是 16384 个桶，也就是 2^14，每个桶的 maxbits 需要 6 个 bits 来存储，最大可以表示 maxbits=63，于是总共占用内存就是`2^14 * 6 / 8 = 12k`字节。

## 布隆过滤

如果我们想要知道某一个值是不是已经在 HyperLogLog 结构里面，它就无能为力了。

如果信息很多的话，使用 set 来判断是否存在很耗空间。布隆过滤器可以用来解决去重问题。它可以在起到去重的同时，在空间上能节省 90% 以上，但是存在误判率，即 某个元素本来不存在，但是误以为已经重复了。

### 布隆过滤器是什么？

**当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在**。打个比方，当它说不认识你时，肯定就不认识；当它说见过你时，可能根本就没见过面，不过因为你的脸跟它认识的人中某脸比较相似 (某些熟脸的系数组合)，所以误判以前见过你。

## 布隆过滤器基本使用

布隆过滤器有二个基本指令，`bf.add` 添加元素，`bf.exists` 查询元素是否存在，它的用法和 set 集合的 sadd 和 sismember 差不多。注意 `bf.add` 只能一次添加一个元素，如果想要一次添加多个，就需要用到 `bf.madd` 指令。同样如果需要一次查询多个元素是否存在，就需要用到 `bf.mexists` 指令。

```shell
127.0.0.1:6379> bf.add codehole user1
(integer) 1
127.0.0.1:6379> bf.add codehole user2
(integer) 1
127.0.0.1:6379> bf.add codehole user3
(integer) 1
127.0.0.1:6379> bf.exists codehole user1
(integer) 1
127.0.0.1:6379> bf.exists codehole user2
(integer) 1
127.0.0.1:6379> bf.exists codehole user3
(integer) 1
127.0.0.1:6379> bf.exists codehole user4
(integer) 0
127.0.0.1:6379> bf.madd codehole user4 user5 user6
1) (integer) 1
2) (integer) 1
3) (integer) 1
127.0.0.1:6379> bf.mexists codehole user4 user5 user6 user7
1) (integer) 1
2) (integer) 1
3) (integer) 1
4) (integer) 0
```

### 注意事项

布隆过滤器的`initial_size`估计的过大，会浪费存储空间，估计的过小，就会影响准确率，用户在使用之前一定要尽可能地精确估计好元素数量，还需要加上一定的冗余空间以避免实际元素可能会意外高出估计值很多。

布隆过滤器的`error_rate`越小，需要的存储空间就越大，对于不需要过于精确的场合，`error_rate`设置稍大一点也无伤大雅。比如在新闻去重上而言，误判率高一点只会让小部分文章不能让合适的人看到，文章的整体阅读量不会因为这点误判率就带来巨大的改变。

### 布隆过滤器的原理

每个布隆过滤器对应到 Redis 的数据结构里面就是一个**大型的位数组和几个不一样的无偏 hash 函数。所谓无偏就是能够把元素的 hash 值算得比较均匀。**

向布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模运算得到一个位置，**每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。**

向布隆过滤器询问 key 是否存在时，跟 add 一样，也会把 hash 的几个位置都算出来，看看位数组中这几个位置是否都为 1，只要有一个位为 0，那么说明布隆过滤器中这个 key 不存在。如果都是 1，这并不能说明这个 key 就一定存在，只是极有可能存在，因为这些位被置为 1 可能是因为其它的 key 存在所致。如果这个位数组比较稀疏，判断正确的概率就会很大，如果这个位数组比较拥挤，判断正确的概率就会降低。

**使用时不要让实际元素远大于初始化大小**，当实际元素开始超出初始化大小时，应该对布隆过滤器进行重建，重新分配一个 size 更大的过滤器，再将所有的历史元素批量 add 进去 (这就要求我们在其它的存储器中记录所有的历史元素)。因为 error_rate 不会因为数量超出就急剧增加，这就给我们重建过滤器提供了较为宽松的时间。

## redis 实现简单限流

系统限定用户的某个行为在**指定的时间里只能运行发送N 次**。

在 redis 中 记录 key (userid + 行为 id )，value  次数  过期时间。

每次	请求到时，递增 value 次数（incr），当value 的值到达限定值的时候，禁止通过，直接返回。

## 漏斗限流

简单限流 限定的是 固定时间里固定的次数，存在一种问题，比如：我限定5s 内最多访问100 次，用户在 前 4 s 没有访问，在最后 1s 一下子访问 100 次。这 100 次请求直接到达下面的业务，可能出现问题。不过这个 可以用 消息队列解决。

漏斗限流是按照常量固定的速率流出请求，而流入的请求速率随意，如果流入的请求超过漏桶容量时，那么再流入的请求便会被丢弃（溢出）。

假设说，我们在压测的时候发现我么你的系统处理的峰值是 100RPS ，超过这个峰值就会出现错误的响应，所以我们在代理层nginx 限制最大请求也为 1000RPS（**设置流水孔的大小**），如果用户每秒的请求超过 1000，nginx 就会暂存这么多请求，排着队等待处理。如果请求量过大，nginx 没法暂存这么多请求，那么多余的请求就会被拒掉，并返回 503 状态码，告知服务器不可用。



```java
long timeStamp=getNowTime();
int capacity;  // 桶的容量
int rate ;     // 水漏出的速度
int water;     // 当前水量

bool grant() {
  //先执行漏水，因为rate是固定的，所以可以认为“时间间隔*rate”即为漏出的水量
  long now = getNowTime();
  water = max(0, water- (now - timeStamp)*rate);
  timeStamp = now;

  if water < capacity { // 水还未满，加水
    water ++;
    return true;
  } else {
    return false;//水满，拒绝加水
  }
}
```

漏桶限流时流出的速率是一定的。也就是业务逻辑 每秒中 最多只会处理这么多请求。

### 令牌桶算法

令牌桶算法是以一个固定的速率王一个固定容量的桶里添加令牌，如果添加的令牌树已经超过桶的容量的话，那么多余的令牌就会被丢弃。当一个请求来临时，就会向桶里要一个令牌，只要拿到令牌，请求就会被处理，正因为这样，令牌桶是可以允许突发请求的。

## Scan

如何从Redis 实例成千上万的 key 中找出成千上万的 key 中找出特定前缀的 key 列表来手动处理数据？

如何从海量的key 中找出满足特定前缀的 key 列表来？

Redis 提供了一个简单暴力的指令 `keys` 用来列出所有满足特定正则字符串规则的 key。

这个指令使用非常简单，提供一个简单的正则字符串即可，但是有很明显的两个**缺点**。

1. 没有 offset、limit 参数，一次性吐出所有满足条件的 key，万一实例中有几百 w 个 key 满足条件，当你看到满屏的字符串刷的没有尽头时，你就知道难受了。
2. keys 算法是遍历算法，**复杂度是 O(n)**，如果实例中有千万级以上的 key，这个指令就会导致 Redis 服务卡顿，所有读写 Redis 的其它的指令都会被延后甚至会超时报错，因为 Redis 是单线程程序，顺序执行所有指令，其它指令必须等到当前的 keys 指令执行完了才可以继续。

Redis 为了解决这个问题，它在 2.8 版本中加入了大海捞针的指令——`scan`。`scan` 相比 `keys`具备有以下特点:

1. 复杂度虽然也是 O(n)，但是它是通过**游标分步进行的，不会阻塞线程;**
2. 提供 limit 参数，可以控制每次返回结果的最大条数，limit 只是一个 hint，返回的结果可多可少;
3. 同 keys 一样，它也提供模式匹配功能;
4. 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数;
5. **返回的结果可能会有重复，需要客户端去重复，这点非常重要;**
6. 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的;
7. 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零;

### scan 基础使用

scan 参数提供了三个参数，第一个是 `cursor 整数值`，第二个是 `key 的正则模式`，第三个是`遍历的 limit hint`。第一次遍历时，cursor 值为 0，然后将**返回结果中第一个整数值**作为下一次遍历的 cursor。一直遍历到返回的 **cursor 值为 0** 时结束。

从上面的过程可以看到虽然提供的 limit 是 1000，但是返回的结果只有 10 个左右。因为这个 limit 不是限定返回结果的数量，而是限定服务器单次遍历的字典槽位数量(约等于)。如果将 limit 设置为 10，你会发现返回结果是空的，但是游标值不为零，意味着遍历还没结束。

### 字典的结构

在 Redis 中所有的 key 都存储在一个很大的字典中，这个字典的结构和 Java 中的 HashMap 一样，是一维数组 + 二维链表结构，第一维数组的大小总是 2^n(n>=0)，扩容一次数组大小空间加倍，也就是 n++。

**scan 指令返回的游标就是第一维数组的位置索引**，我们将这个位置索引称为槽 (slot)。如果不考虑字典的扩容缩容，直接按数组下标挨个遍历就行了。limit 参数就表示需要遍历的槽位数，**之所以返回的结果可能多可能少，是因为不是所有的槽位上都会挂接链表，有些槽位可能是空的，还有些槽位上挂接的链表上的元素可能会有多个**。每一次遍历都会将 limit 数量的槽位上挂接的所有链表元素进行模式匹配过滤后，一次性返回给客户端。

### scan 遍历顺序

scan 的遍历顺序非常特别。它不是从第一维数组的第 0 位一直遍历到末尾，而是采用了**高位进位加法**来遍历。之所以使用这样特殊的方式进行遍历，是考虑到字典的扩容和缩容时避免槽位的遍历重复和遗漏。

![img](https://user-gold-cdn.xitu.io/2018/7/5/16469760d12e0cbd?imageslim)

采用高位进位加法的遍历顺序，rehash 后的槽位在遍历顺序上是相邻的。



## Redis 线程IO 模型

**Redis 是个单线程程序**！这点必须铭记。这里强调的 单线程，只是处理我们的网络请求的时候只有一个线程处理（处理客户端的请求，例如 set key get key ），但是 一个 Redis Server 运行的时候不止一个线程，例如 Redis 进行持久化的时候会以子进程或子线程的方式执行。

也许你会怀疑高并发的 Redis 中间件怎么可能是单线程。很抱歉，它就是单线程，你的怀疑暴露了你基础知识的不足。莫要瞧不起单线程，除了 Redis 之外，Node.js 也是单线程，Nginx 也是单线程，但是它们都是服务器高性能的典范。

> 为什么要使用多线程？
>
> 多线程的原因 一般是 cpu  资源时瓶颈，需要让 cpu 资源跑满。而在很多情况下，cpu 并不是瓶颈，所以使用 cpu 意义不大，并且有可能增加消耗。
>
> 为什么使用单线程？
>
> 因为 Redis 是基于内存的操作，Cpu 不是 Redis 的瓶颈，Redis 的瓶颈最有可能的是机器内存的大小或者网络带宽吗，采用多线程 的话 需要考虑各种锁的问题，比较麻烦。

### 为什么 Redis 单线程还能这么快？

因为它所有的数据都在内存中，所有的运算都是内存级别的运算。正因为 Redis 是单线程，所以要小心使用 Redis 指令，对于那些时间复杂度为 O(n) 级别的指令，一定要谨慎使用，一不小心就可能会导致 Redis 卡顿。

### 单线程如何处理那么多的并发客户端连接？

使用多路复用

> 多路复用是什么？多路复用的本质是什么？epoll 比select 高效原因是什么？
>
> ### 一、 从网卡接收数据说起
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709123302.jpg)
>
> 上图是典型的计算机结构图，计算机由CPU、存储器与网络接口等部件组成，我们先从硬件的角度看计算机怎样接收网络数据。
>
> 网卡接收数据的过程：
>
> - 首先，网卡收到网线传来数据；（网卡上的传感器检测到电信号？）
> - 网卡上的控制器，通过 DMA 传输、IO通路选择等 将接收到的数据**写入到内存**。
>
> ### 二、如何知道接收了数据
>
> **从 cpu  角度来看数据接收**，首先要理解一个概念--中断。
>
> 计算机执行程序时，会有优先级的要求，比如，当计算机收到断电信号时，它应立即去保存数据，保存数据的程序具有较高的优先级。
>
> 一般而言，由硬件产生的信号需要 CPU 立马做出回应，不然数据可能就丢失了，所以它的优先级很高。CPU 理应中断掉正在执行的程序，去做出响应，去执行相应的中断程序；当 CPU 完成对硬件的响应后，再重新执行用户程序。它和函数调用差不多，只不过函数调用是事先定好位置，而中断的位置由“信号”决定。
>
> 以键盘为例，当用户按下键盘某个按键时，键盘会给CPU的中断引脚发出一个**高电平**，CPU 能捕获这个信号，然后执行键盘中断程序。
>
> 现在可以回答“**如何知道接收了数据？**”这个问题了：**当网卡把数据写入到内存后，网卡向 CPU 发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据。**
>
> ### 三、 进程阻塞为什么不占用 cpu 资源？
>
> **从操作系统进程调度的角度来看数据接收**。阻塞是进程调度的关键一环，指的是进程在等待某事件发生之前的等待状态，recv 和 epoll 都是阻塞方法。
>
> 从普通的 recv 接收开始分析，先看看下面的代码：
>
> ```java
> //创建socket
> int s = socket(AF_INET, SOCK_STREAM, 0);   
> //绑定
> bind(s, ...)
> //监听
> listen(s, ...)
> //接受客户端连接
> int c = accept(s, ...)
> //接收客户端数据
> recv(c, ...);
> //将数据打印出来
> printf(...)
> ```
>
> 这是一段最基础的网络编程代码，先新建 socket 对象，依次调用 bind、listen 与 accept，最后调用 recv 接收数据。recv 是个阻塞方法，当程序运行到 recv 时，它会一直等待，直到接收到数据才往下执行。
>
> **阻塞的原理是什么？**
>
> *工作队列*
>
> 操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“**运行**”和“**等待**”等几种状态。运行状态是进程获得 CPU 使用权，正在执行代码的状态；等待状态是阻塞状态，比如上述程序运行到 recv 时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态。操作系统会分时执行各个**运行状态**的进程，由于速度很快，看上去就像是同时执行多个任务。
>
> **可以理解为 操作系统的内核空间 维护了 一个工作队列，队列中 都是处在运行状态的进程（可以获得 cpu 使用权的进程）。**
>
> 
>
> *等待队列*
>
> 当进程 A 执行到创建 socket 的语句时，操作系统会创建一个由**文件系统管理的 socket 对象**（如下图，**PCB** （进程控制块）记录了进程的相关信息，一个进程维护了一个文件描述符表。*每一个文件描述符会与一个打开文件相对应，同时，不同的文件描述符也会指向同一个文件。相同的文件可以被不同的进程打开也可以在同一个进程中被多次打开。系统为每一个进程维护了一个文件描述符表，该表的值都是从0开始的，所以在不同的进程中你会看到相同的文件描述符，这种情况下相同文件描述符有可能指向同一个文件，也有可能指向不同的文件*）。**这个 socket 对象包含了发送缓冲区、接收缓冲区与等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该 socket 事件的进程。**
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709130848.jpg)
>
> 当程序 执行到 recv 时，操作系统会将进程A 从工作队列移动到 该 socket 的等待队列中。由于工作队列只剩下 进程B 和C，依据进程调度，CPU会轮流执行这两个进程的程序，不会执行进程A的程序，所以进程A 被阻塞，不会往下执行代码，也不会占用CPU 资源。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709131233.jpg)
>
> *注：操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。上图为了方便说明，直接将进程挂到等待队列之下。*
>
> 
>
> *唤醒进程*
>
> 当 socket 接收到数据后，操作系统将该 socket 等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。**同时由于 socket 的接收缓冲区已经有了数据**，recv 可以返回接收到的数据。
>
> ### 四、内核接收网络数据全过程
>
> 如下图所示，进程在 recv 阻塞期间，计算机收到了对端传送的数据（**步骤①**），数据经由网卡传送到内存（**步骤②**，放在内存中网卡缓冲区），然后网卡通过中断信号通知 CPU 有数据到达，CPU 执行中断程序（步骤③）。
>
> 此处的中断程序主要有两项功能，**先将网络数据写入到对应 socket 的接收缓冲区里面（步骤④），再唤醒进程 A（步骤⑤），重新将进程 A 放入工作队列中**。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709131611.jpg)
>
> 唤醒进程的过程如下图所示：
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709131906.jpg)
>
> 以上是内核接收数据全过程，这里我们可能会思考两个问题：
>
> - 其一，操作系统如何知道网络数据对应于哪个 socket？
> - 其二，如何同时监视多个 socket 的数据？
>
> 第一个问题：因为一个 socket 对应着一个端口号，**而网络数据包中包含了 ip 和端口的信息，内核可以通过端口号找到对应的 socket**。当然，为了提高处理速度，操作系统会维护端口号到 socket 的索引结构，以快速读取。
>
> 第二个问题是多路复用的重中之重，也正是本文后半部分的重点。
>
> ### 五、同时监视多个socket 的简单方法
>
> 服务端需要管理多个客户端连接，而 recv 只能监视单个 socket，这种矛盾下，人们开始寻找监视多个 socket 的方法。**epoll 的要义就是高效地监视多个 socket**。
>
> 从历史发展角度看，必然先出现一种不太高效的方法，人们再加以改进，正如 select 之于 epoll。
>
> 先理解不太高效的 select，才能够更好地理解 epoll 的本质。
>
> 假如能够预先传入一个 socket 列表，如果列表中的 socket 都没有数据，挂起进程，直到有一个 socket 收到数据，唤醒进程。这种方法很直接，也是 select 的设计思想。
>
> 为方便理解，我们先复习 select 的用法。在下边的代码中，先准备一个数组 fds，让 fds 存放着所有需要监视的 socket。然后调用 select，如果 fds 中的所有 socket 都没有数据，select 会阻塞，直到有一个 socket 接收到数据，select 返回，唤醒进程。用户可以遍历 fds，**通过 FD_ISSET 判断具体哪个 socket 收到数据，然后做出处理**。
>
> ```JAVA
> int s = socket(AF_INET, SOCK_STREAM, 0);  
> bind(s, ...);
> listen(s, ...);
> int fds[] =  存放需要监听的socket;
> while(1){
>     int n = select(..., fds, ...)
>     for(int i=0; i < fds.count; i++){
>         if(FD_ISSET(fds[i], ...)){
>             //fds[i]的数据处理
>         }
>     }}
> ```
>
> **select 的流程**
>
> select 的实现思路很直接，假如程序同时监视如下图的 sock1、sock2 和 sock3 三个 socket，那么在调用 select 之后，操作系统把进程 A 分别加入这三个 socket 的等待队列中。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709132640.jpg)
>
> 当任何一个 socket 收到数据后，中断程序将唤起进程。下图展示了 sock2 接收到了数据的处理流程：
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709132739.jpg)
>
> 所谓唤起进程，就是将 进程从**所有的等待队列中移除**，加入到工作队列里面。
>
> 经由这些步骤，当进程 A 被唤醒后，它知道至少有一个 socket 接收了数据。程序只需遍历一遍 socket 列表，就可以得到就绪的 socket。
>
> 这种简单方式行之有效，在几乎所有操作系统都有对应的实现。
>
> 但是简单的方法往往有缺点，主要是：
>
> 其一，每次调用 select **都需要将进程加入到所有监视 socket 的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历**，而且每次都要将整个 fds 列表传递给内核（从用户空间传到内核空间），有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定 select 的最大监视数量，默认只能监视 1024 个 socket。
>
> 其二，进程被唤醒后，程序并不知道哪些 socket 收到数据，还需要遍历一次。
>
> 那么，**有没有减少遍历的方法（三次遍历，1. 将进程加入到所有socket的等待队列 2. 将进程从所有socket 的等待队列移除 3. 想知道那些 socket 收到数据，遍历一次）？有没有保存就绪 socket 的方法？这两个问题便是 epoll 技术要解决的**。
>
> ### 六、epoll 的设计思路
>
> epoll 是在 select 出现 N 多年后才被发明的，是 select 和 poll（poll 和 select 基本一样，有少量改进）的增强版本。epoll 通过以下一些措施来改进效率：
>
> **措施一：功能分离**
>
> select 低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用 select 都需要这两步操作，然而大多数应用场景中，需要监视的 socket 相对固定，并不需要每次都修改。epoll 将这两个操作分开，**先用 epoll_ctl 维护等待队列**，**再调用 epoll_wait 阻塞进程**。显而易见地，效率就能得到提升。
>
> 为方便理解后续的内容，我们先了解一下 epoll 的用法。如下的代码中，先用 epoll_create 创建一个 epoll 对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到 epfd 中，最后调用 epoll_wait 等待数据：
>
> ```java
> int s = socket(AF_INET, SOCK_STREAM, 0);   
> bind(s, ...)
> listen(s, ...)
> 
> int epfd = epoll_create(...);
> epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中
> 
> while(1){
>     int n = epoll_wait(...)
>     for(接收到数据的socket){
>         //处理
>     }
> }
> ```
>
> **措施二：就绪列表**
>
> select 低效的另一个原因在于程序不知道哪些 socket 收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的 socket，就能避免遍历。如下图所示，计算机共有三个 socket，收到数据的 sock2 和 sock3 被就绪列表 rdlist 所引用。当进程被唤醒后，只要获取 rdlist 的内容，就能够知道哪些 socket 收到数据。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709133958.jpg)
>
> ### 七、epoll 的原理与工作流程
>
> **创建 epoll 对象**
>
> 如下图所示，当某个进程调用 epoll_create 方法时，内核会创建一个 **eventpoll** 对象（也就是程序中 epfd 所代表的对象）。**eventpoll** 对象也是文件系统中的一员，和 socket 一样，它也会有等待队列。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709134105.jpg)
>
> 创建一个代表该 epoll 的 eventpoll 对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为 eventpoll 的成员。
>
> **维护监视列表**
>
> 创建 epoll 对象后，可以用 epoll_ctl 添加或删除所要监听的 socket。以添加 socket 为例，如下图，如果通过 epoll_ctl 添加 sock1、sock2 和 sock3 的监视，**内核会将 eventpoll 添加到这三个 socket 的等待队列中。**
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709134153.jpg)
>
> 当 socket 收到数据后，中断程序会操作 eventpoll 对象，而不是直接操作进程。
>
> **接收数据**
>
> 当 socket 收到数据后，**中断程序会给 eventpoll 的“就绪列表”添加 socket 引用**。如下图展示的是 sock2 和 sock3 收到数据后，中断程序让 rdlist 引用这两个 socket。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709134408.jpg)
>
> eventpoll 对象相当于 socket 和进程之间的中介，socket 的数据接收并不直接影响进程，而是通过改变 eventpoll 的就绪列表来改变进程状态。
>
> 当程序执行到 epoll_wait 时，如果 rdlist 已经引用了 socket，那么 epoll_wait 直接返回，如果 rdlist 为空，阻塞进程。
>
> **阻塞和唤醒进程**
>
> 假设计算机中正在运行进程 A 和进程 B，在某时刻进程 A 运行到了 epoll_wait 语句。如下图所示，内核会将进程 A 放入 eventpoll 的等待队列中，阻塞进程。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709134603.jpg)
>
> 当 socket 接收到数据，中断程序一方面修改 rdlist，**另一方面唤醒 eventpoll 等待队列中的进程，进程 A 再次进入运行状态（如下图）**。也因为 rdlist 的存在，进程 A 可以知道哪些 socket 发生了变化。
>
> ### 八、epoll 的实现细节
>
> 我们还需要知道 eventpoll 的数据结构是什么样子？
>
> 此外，就绪队列应该应使用什么数据结构？eventpoll 应使用什么数据结构来管理通过 epoll_ctl 添加或删除的 socket？
>
> 如下图所示，eventpoll 包含了 lock、mtx、wq（等待队列）与 rdlist 等成员，其中 rdlist 和 rbr 是我们所关心的。
>
> ![](C:\Users\Administrator\Documents\笔记\img\微信图片_20190709134842.jpg)
>
> **就绪列表的数据结构**
>
> 就绪列表引用着就绪的 socket，所以它应能够快速的插入数据。
>
> 程序可能随时调用 epoll_ctl 添加监视 socket，也可能随时删除。当删除时，若该 socket 已经存放在就绪列表中，它也应该被移除。**所以就绪列表应是一种能够快速插入和删除的数据结构**。
>
> 双向链表就是这样一种数据结构，epoll 使用双向链表来实现就绪队列（对应上图的 rdllist）。
>
> **索引结构**
>
> 既然 epoll 将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的 socket，至少要方便地添加和移除，还要便于搜索，以避免重复添加。**红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好，epoll 使用了红黑树作为索引结**构（对应上图的 rbr）。
>
> 注：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist 并非直接引用 socket，而是通过 epitem 间接引用，红黑树的节点也是 epitem 对象。同样，文件系统也并非直接引用着 socket。为方便理解，本文中省略了一些间接结构。



###  指令队列

Redis 会将每个客户端套接字都关联一个 指令队列。客户端的指令通过队列来排队进行顺序处理，先到先服务。

### 响应队列

Redis 同样也会为每个客户端套接字关联一个响应队列。Redis 服务器通过响应队列来将指令的返回结果回复给客户端。 如果队列为空（指令队列？），那么意味着连接暂时处于空闲状态，不需要去获取写事件，也就是可以将当前的客户端描述符从`write_fds`里面移出来。等到队列有数据了，再将描述符放进去。避免`select`系统调用立即返回写事件，结果发现没什么数据可以写。出这种情况的线程会飙高 CPU。

### 定时任务

服务器处理要响应 IO 事件外，还要处理其它事情。比如定时任务就是非常重要的一件事。如果线程阻塞在 select 系统调用上，定时任务将无法得到准时调度。那 Redis 是如何解决这个问题的呢？

Redis 的定时任务会记录在一个称为`最小堆`的数据结构中。这个堆中，最快要执行的任务排在堆的最上方。在每个循环周期，Redis 都会将最小堆里面已经到点的任务立即进行处理。**处理完毕后，将最快要执行的任务还需要的时间记录下来，这个时间就是`select`系统调用的`timeout`参数**。（Netty 也是这样 做的）因为 Redis 知道未来`timeout`时间内，没有其它定时任务需要处理，所以可以安心睡眠`timeout`的时间。

## 通信协议

Redis 的作者任务数据库系统的瓶颈一般不在于 网络流量，而是数据库自身内部逻辑处理上。所以即使Redis 使用了浪费流量的文本协议，依然可以取得极高的 访问性能。

### RESP(Redis Serialization Protocol)

RESP 是 Redis 序列化协议的简写。它是一种直观的文本协议，优势在于实现异常简单，解析性能极好。

Redis 协议将传输的结构数据分为 5 种最小单元类型，单元结束时统一加上回车换行符号`\r\n`。

1. 单行字符串 以 `+` 符号开头。
2. 多行字符串 以 `$` 符号开头，后跟字符串长度。
3. 整数值 以 `:` 符号开头，后跟整数的字符串形式。
4. 错误消息 以 `-` 符号开头。
5. 数组 以 `*` 号开头，后跟数组的长度。

### 客户端向服务器

客户端向服务器发送的指令只有一种格式，**多行字符串数组**。比如一个简单的 set 指令`set author codehole`会被序列化成下面的字符串。

```shell
*3\r\n$3\r\nset\r\n$6\r\nauthor\r\n$8\r\ncodehole\r\n
//在控制台输出这个字符串如下，可以看出这是很好阅读的一种格式
*3
$3
set
$6
author
$8
codehole
```

### 服务器到客户端

服务器向客户端回复的响应要支持多种数据结构，所以消息响应在结构上要复杂不少。不过再复杂的响应消息也是以上 5 中基本类型的组合。

## 持久化

Redis 的数据全部在内存里，如果突然宕机，数据就会全部丢失，因此必须有一种机制来保证 Redis 的数据不会因为故障而丢失，这种机制就是 Redis 的持久化机制。

Redis 的持久化机制有两种，第一种是快照，第二种是 AOF 日志。快照是一次全量备份，AOF 日志是连续的增量备份。**快照是内存数据的二进制序列化形式，在存储上非常紧凑，而 AOF 日志记录的是内存数据修改的指令记录文本。**AOF 日志在长期的运行过程中会变的无比庞大，数据库重启时需要加载 AOF 日志进行指令重放，这个时间就会无比漫长。所以需要定期进行 AOF 重写，给 AOF 日志进行瘦身。

- RDB：RDB 持久化机制，是对 redis 中的数据执行**周期性**的持久化。
- AOF：AOF 机制对每条写入命令作为日志，以 `append-only` 的模式写入一个日志文件中，在 redis 重启的时候，可以通过**回放** AOF 日志中的写入指令来重新构建整个数据集。

|              RDB持久化              |                      AOF持久化                      |
| :---------------------------------: | :-------------------------------------------------: |
|     全量备份,一次保存整个数据库     |        增量备份,一次保存一个修改数据库的命令        |
|           保存的间隔较长            |                 保存的间隔默认一秒                  |
|           数据还原速度快            |                  数据还原速度一般                   |
| save会阻塞,但bgsave或者自动不会阻塞 |          无论是平时还是AOF重写,都不会阻塞           |
|       更适合数据备份,默认开启       | 更适合用来保存数据,和一般SQL持久化方式一样,默认关闭 |
|           启动优先级 : 低           |                   启动优先级 : 高                   |
|              体积 : 小              |                      体积 : 大                      |
|            恢复速度 : 快            |                    恢复速度 : 慢                    |
|         数据安全性 : 丢数据         |              数据安全性 : 根据策略决定              |
|              轻重 : 重              |                      轻重 : 轻                      |



### 快照原理

我们知道 Redis 是单线程程序，这个线程要同时负责多个客户端套接字的并发读写操作和内存数据结构的逻辑读写。

在服务线上请求的同时，Redis 还需要进行内存快照，内存快照要求 Redis 必须进行文件 IO 操作，可文件 IO 操作是不能使用多路复用 API（**文件IO 可以使用多路复用API，这里开多个线程的原因是因为会阻塞 业务的执行**）。

还要一个问题是：如果一边持久化边响应客户端的请求，持久化的同时，内存数据结构还在改变，怎么办？

**Redis 使用操作系统的 多进程 COW（Copy On Write） 机制 来实现快照持久化。**

#### fork 

Redis 在持久化时会调用 glibc 的函数 fork 产生一个 子进程，快照持久化完成交给子进程来处理，父进程继续处理客户端请求。子进程刚刚产生时，它和父进程共享内存里面的代码段和数据段。这时你可以将父子进程想像成一个连体婴儿，共享身体。这是 Linux 操作系统的机制，为了节约内存资源，所以尽可能让它们共享起来。在进程分离的一瞬间，内存的增长几乎没有明显变化。



子进程做数据持久化，它不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化写到磁盘中。但是父进程不一样，它必须持续服务客户端请求，然后对内存数据结构进行不间断的修改。

这个时候就会使用操作系统的 **COW** 机制来进行数据段页面的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生时那一瞬间的数据。

随着父进程修改操作的持续进行，越来越多的共享页面被分离出来，内存就会持续增长。但是也不会超过原有数据内存的 2 倍大小。**另外一个 Redis 实例里冷数据占的比例往往是比较高的，所以很少会出现所有的页面都会被分离，被分离的往往只有其中一部分页面**。每个页面的大小只有 4K，一个 Redis 实例里面一般都会有成千上万的页面。

子进程因为数据没有变化，它能看到的内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也是为什么 Redis 的持久化叫「快照」的原因。接下来子进程就可以非常安心的遍历数据了进行序列化写磁盘了。

### AOF 原理

> Append Only File 

**AOF 日志存储的是 Redis 服务器的顺序指令序列，AOF 日志只记录对内存进行修改的指令记录**。

假设 AOF 日志记录了自 Redis 实例创建以来所有的修改性指令序列，那么就可以通过对一个空的 Redis 实例顺序执行所有的指令，也就是「重放」，来恢复 Redis 当前实例的内存数据结构的状态。

Redis 会在收到客户端修改指令后，进行参数校验进行逻辑处理后，如果没问题，就立即将该指令文本存储到 AOF 日志中，**也就是先执行指令才将日志存盘**（AOF 里存的是被执行的指令）。这点不同于leveldb、hbase等存储引擎，它们都是先存储日志再做逻辑处理。

Redis 在长期运行的过程中，AOF 的日志会越变越长。如果实例宕机重启，重放整个 AOF 日志会非常耗时，导致长时间 Redis 无法对外提供服务。所以需要对 AOF 日志瘦身。

AOF的执行流程包括： 

- 命令追加(append)：将Redis的写命令追加到缓冲区aof_buf； 
- 文件写入(write)和文件同步(sync)：根据不同的同步策略将aof_buf中的内容同步到硬盘； 
- 文件重写(rewrite)：定期重写AOF文件，达到压缩的目的。

### AOF 重写

Redis 提供了 bgrewriteaof 指令用于对 AOF 日志进行瘦身。**其原理就是开辟一个子进程对内存进行遍历转换成一系列 Redis 的操作指令，序列化到一个新的 AOF 日志文件中**。序列化完毕后再将操作期间发生的增量 AOF 日志追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件了，瘦身工作就完成了。

### fsync

AOF 日志是以文件的形式存在的，当程序对AOF 日志文件进行写操作时，实际上是将内容写到了 内核为文件描述符分配的一个内存缓存中，然后内核会异步将脏数据刷回到磁盘的。

Linux 的`glibc`提供了`fsync(int fd)`函数可以将指定文件的内容强制从内核缓存刷到磁盘。只要 Redis 进程实时调用 fsync 函数就可以保证 aof 日志不丢失。但是 fsync 是一个磁盘 IO 操作，它很慢！如果 Redis 执行一条指令就要 fsync 一次，那么 Redis 高性能的地位就不保了。

所以在生产环境的服务器中，Redis 通常是每隔 1s 左右执行一次 fsync 操作，周期 1s 是可以配置的。这是在数据安全性和性能之间做了一个折中，在保持高性能的同时，尽可能使得数据少丢失。

Redis 同样也提供了另外两种策略，一个是永不 fsync——让操作系统来决定何时同步磁盘，很不安全，另一个是来一个指令就 fsync 一次——非常慢。但是在生产环境基本不会使用，了解一下即可。

在 redis 的较新版本中（不知道从哪个版本开始） 增加了两个新的子进程

- REDIS_BIO_CLOSE_FILE 负责所有的 close file 操作
- REDIS_BIO_AOF_FSYNC 负责fsync 操作

因为这两个操作都可能会引起阻塞，如果在主线程中完成话，会影响系统对时间的响应。所以这里统一由相应的线程来完成，每个线程都有一个自己的*bio_jobs list*，用来保存需要的处理的job任务。

### RDB与AOF 的优缺点

#### RDB 优缺点

- RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，**非常适合做冷备**，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。

- RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis **保持高性能**，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。

- 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。

- 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是**每隔 5 分钟**，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失**最近 5 分钟**的数据。

- RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。

  

  #### AOF 优缺点

  - AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次`fsync`操作，最多丢失 1 秒钟的数据。
  - AOF 日志文件以 `append-only` 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。
  - AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 `rewrite` log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。
  - AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常**适合做灾难性的误删除的紧急恢复**。比如某人不小心用 `flushall` 命令清空了所有数据，只要这个时候后台 `rewrite` 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 `flushall` 命令给删了，然后再将该 `AOF` 文件放回去，就可以通过恢复机制，自动恢复所有数据。
  - 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。
  - AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 `fsync` 一次日志文件，当然，每秒一次 `fsync`，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低）
  - 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是**基于当时内存中的数据进行指令的重新构建**，这样健壮性会好很多。

## 主从同步

用 Redis 的集群，那么当 master 挂掉的时候，让从库过来接管，服务可以继续。

### CAP

- **C** - Consistent ，一致性
- **A** - Availability ，可用性
- **P** - Partition tolerance ，分区容忍性

分布式系统的节点往往都是 分布在不同的机器上进行网络隔离的，这意味着必然会存在 **网络分区**。

在网络分区发生时，两个分布式节点之间无法进行通信，所以数据的 **一致性** 将无法满足。除非我们 牺牲 **可用性**，也就是暂停分布式节点服务，在网络分区发生时，不再提供修改数据的功能，直到网络状况完全恢复正常再继续对外提供服务。

一句话概括 CAP 原理就是——**网络分区发生时，一致性和可用性两难全**。

### 最终一致

Redis 的主从数据是异步同步的，所以分布式的 Redis 系统并不满足「**一致性**」要求。当客户端在 Redis 的主节点修改了数据后，立即返回，即使在主从网络断开的情况下，主节点依旧可以正常对外提供修改服务，所以 Redis 满足「**可用性**」。

Redis 保证「**最终一致性**」，从节点会努力追赶主节点，最终从节点的状态会和主节点的状态将保持一致。如果网络断开了，主从节点的数据将会出现大量不一致，一旦网络恢复，**从节点会采用多种策略努力追赶上落后的数据，继续尽力保持和主节点一致。**

### 主从同步

![img](https://user-gold-cdn.xitu.io/2018/7/4/164641d454a0e67a?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

### 增量同步

Redis 同步的是 **指令流**，主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存 buffer 中，**然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了 (偏移量)**。

因为内存的 buffer 是有限的，所以 Redis 主库不能将所有的指令都记录在内存 buffer 中。Redis 的复制内存 buffer 是一个定长的环形数组，如果数组内容满了，就会从头开始覆盖前面的内容。

如果因为网络状况不好，从节点在短时间内无法和主节点进行同步，那么当网络状况恢复时，Redis 的主节点中那些没有同步的指令在 buffer 中有可能已经被后续的指令覆盖掉了，从节点将无法直接通过指令流来进行同步，这个时候就需要用到更加复杂的同步机制 —— 快照同步。



### 快照同步

快照同步是一个非常耗费资源的操作，它首先需要在主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。从**节点将快照文件接受完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空。加载完毕后通知主节点继续进行增量同步。**

在整个快照同步进行的过程中，主节点的复制 buffer 还在不停的往前移动，如果快照同步的时间过长或者复制 buffer 太小，都会导致同步期间的增量指令在复制 buffer 中被覆盖，这样就会导致快照同步完成后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的死循环。

所以务必配置一个合适的复制 buffer 大小参数，避免快照复制的死循环。

### 增加从节点

当从节点刚刚加入到集群时，它必须先要进行一次**快照同步**，同步完成后再继续进行增量同步。

### 无盘复制

主节点 再进行快照同步时，会进行很重的文件 IO 操作，所谓无盘复制是指主服务器直接通过套接字将快照内容发送到从节点，生成快照是一个遍历的过程，主节点会一边遍历内存，一边将序列化的内容发送到从节点，从节点还是跟之前一样，先将接收到的内容存储到磁盘文件中，再进行一次性加载。即主节点 的生成快照内容 不经过磁盘，直接传输，

## LRU

当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)。交换会让 Redis 的性能急剧下降，对于访问量比较频繁的 Redis 来说，这样龟速的存取效率基本上等于不可用。

在生产环境中我们是不允许 Redis 出现交换行为的，为了限制最大使用内存，Redis 提供了配置参数 `maxmemory` 来限制内存超出期望大小。

当实际内存超出 `maxmemory` 时，Redis 提供了几种可选策略 (maxmemory-policy) 来让用户自己决定该如何腾出新的空间以继续提供读写服务。



**noeviction** 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。

**volatile-lru** **尝试淘汰设置了过期时间的 key**，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。

**volatile-ttl** 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，**ttl 越小越优先被淘汰**。

**volatile-random** 跟上面一样，**不过淘汰的 key 是过期 key 集合中随机的 key**。

**allkeys-lru** 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。

**allkeys-random** 跟上面一样，不过淘汰的策略是随机的 key。

volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。



### LRU 算法

实现 LRU 算法除了需要 key/value 字典外，还需要附加一个双向链表，链表中的元素按照一定的顺序进行排列。当空间满的时候，会踢掉链表尾部的元素。当字典的某个元素被访问时，它在链表中的位置会被移动到表头。所以链表的元素排列顺序就是元素最近被访问的时间顺序。

位于链表尾部的元素就是不被重用的元素，所以会被踢掉。位于表头的元素就是最近刚刚被人用过的元素，所以暂时不会被踢。

### 近似 LRU 算法

Redis 使用的是一种近似 LRU 算法，它跟 LRU 算法还不太一样。之所以不使用 LRU 算法，是因为需要消耗大量的额外的内存，需要对现有的数据结构进行较大的改造。近似 LRU 算法则很简单，在现有数据结构的基础上使用**随机采样法来淘汰元素**，能达到和 LRU 算法非常近似的效果。Redis 为实现近似 LRU 算法，它给每个 key 增加了一个**额外的小字段**，这个字段的长度是 24 个 bit，也就是最后一次被访问的时间戳。

上一节提到处理 key 过期方式分为集中处理和懒惰处理，LRU 淘汰不一样，它的处理方式只有懒惰处理。当 Redis 执行写操作时，发现内存超出 maxmemory，就会执行一次 LRU 淘汰算法。这个算法也很简单，**就是随机采样出 5(可以配置) 个 key，然后淘汰掉最旧的 key**，如果淘汰后内存还是超出 maxmemory，那就继续随机采样淘汰，直到内存低于 maxmemory 为止。

## 过期策略

### 过期集合

redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，以后会定时遍历这个字典来删除到期的 key。除了定时遍历之外，它还会使用惰性策略来删除过期的 key，**所谓惰性策略就是在客户端访问这个 key 的时候，redis 对 key 的过期时间进行检查，如果过期了就立即删除。定时删除是集中处理，惰性删除是零散处理。**

### 定时扫描策略



Redis 默认会每秒进行十次过期扫描，过期扫描不会遍历过期字典中所有的 key，而是采用了一种简单的贪心策略。

1. 从过期字典中随机 20 个 key；
2. 删除这 20 个 key 中已经过期的 key；
3. 如果过期的 key 比率超过 1/4，那就重复步骤 1；

同时，为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms。

设想一个大型的 Redis 实例中所有的 key 在同一时间过期了，会出现怎样的结果？

毫无疑问，Redis 会持续扫描过期字典 (循环多次)，直到过期字典中过期的 key 变得稀疏，才会停止 (循环次数明显下降)。这就会导致线上读写请求出现明显的卡顿现象。导致这种卡顿的另外一种原因是内存管理器需要频繁回收内存页，这也会产生一定的 CPU 消耗。

**当客户端请求到来时，服务器如果正好进入过期扫描状态，客户端的请求将会等待至少 25ms 后才会进行处理，如果客户端将超时时间设置的比较短，比如 10ms，那么就会出现大量的链接因为超时而关闭，业务端就会出现很多异常。而且这时你还无法从 Redis 的 slowlog 中看到慢查询记录，因为慢查询指的是逻辑处理过程慢，不包含等待时间。**

所以业务开发人员一定要注意过期时间，如果有大批量的 key 过期，要给过期时间设置一个随机范围，而不宜全部在同一时间过期，分散过期处理的压力。

> 在一些活动系统中，因为活动是一期一会，下一期活动举办时，前面几期的很多数据都可以丢弃了，所以需要给相关的活动数据设置一个过期时间，以减少不必要的 Redis 内存占用。如果不加注意，你可能会将过期时间设置为活动结束时间再增加一个常量的冗余时间，如果参与活动的人数太多，就会导致大量的 key 同时过期。